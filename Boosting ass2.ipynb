{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b6f632-8228-4b37-acd8-47b00697c4cf",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d3421-32ac-41e6-8234-e5b087cc7859",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous value. It is part of the broader class of ensemble methods, which combine the predictions of multiple models to achieve better performance than individual models. In Gradient Boosting Regression, the idea is to build a series of weak learners, typically decision trees, in a sequential manner. Each new tree focuses on correcting the errors (residuals) made by the previous trees.\n",
    "\n",
    "Working:\n",
    "\n",
    "1. Initial Model: The process begins with an initial model, often just the mean of the target values.\n",
    "\n",
    "2. Residual Calculation: For each training sample, the residual (error) is calculated, which is the difference between the actual target value and the prediction from the current model.\n",
    "\n",
    "3. Fit Weak Learner: A new weak learner (usually a shallow decision tree) is trained to predict these residuals (the errors of the current model).\n",
    "\n",
    "4. Update the Model: The predictions from the new weak learner are added to the current model's predictions, with a learning rate applied to control the contribution of each new tree.\n",
    "\n",
    "5. Repeat: This process is repeated iteratively. In each iteration, a new tree is added, and the model gets progressively better at predicting the target values by focusing on the errors of previous iterations.\n",
    "\n",
    "6. Stop: The process stops after a predefined number of iterations or when adding new trees does not improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bea723-2803-4f9a-9ced-ab03da0f4340",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85b482d-8b03-4258-b7a1-e0d9d3a0b43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0103999999999993, 0.0, array([3.06, 3.06, 3.06, 3.06, 3.06]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simple dataset (X: input features, y: target values)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1.2, 1.9, 3.0, 4.1, 5.1])\n",
    "\n",
    "# Gradient Boosting Regressor from scratch\n",
    "class GradientBoostingRegressorScratch:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def _fit_base_model(self, residuals):\n",
    "        # A simple decision stump (1-level decision tree) for demonstration purposes\n",
    "        mean_residual = np.mean(residuals)\n",
    "        return mean_residual\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean prediction\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full_like(y, self.initial_prediction)\n",
    "        self.models = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals (errors from the current model)\n",
    "            residuals = y - predictions\n",
    "            # Fit a base model (mean of residuals for simplicity)\n",
    "            model = self._fit_base_model(residuals)\n",
    "            self.models.append(model)\n",
    "            # Update the predictions by adding the new model's contribution\n",
    "            predictions += self.learning_rate * model\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial prediction\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        # Add the contribution from each weak learner\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model\n",
    "        return predictions\n",
    "\n",
    "# Initialize and train the model\n",
    "gbr_scratch = GradientBoostingRegressorScratch(n_estimators=10, learning_rate=0.1)\n",
    "gbr_scratch.fit(X, y)\n",
    "\n",
    "# Predict on training data\n",
    "y_pred = gbr_scratch.predict(X)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "mse, r2, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f97bc1-4166-4e54-8f77-0b0ddc93b5c4",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE): 2.01\n",
    "\n",
    "R-squared (RÂ²): 0.0\n",
    "\n",
    "The predicted values for all inputs were approximately 3.06, which indicates that the model is underfitting. This happens because the current base learner in this example is too simplistic (a constant mean model), and more advanced methods (like decision trees) are needed for a better fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cf43d-fa17-4345-acf0-59b3db657538",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce79121-73a4-49ea-b3e9-cc13229fe4da",
   "metadata": {},
   "source": [
    "\n",
    "To optimize the performance of the Gradient Boosting model, we can experiment with different hyperparameters such as:\n",
    "\n",
    "1. Learning Rate: Controls the contribution of each tree.\n",
    "2. Number of Trees (n_estimators): The number of weak learners.\n",
    "3. Tree Depth (max_depth): The complexity of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3cf98d-af94-4294-8ea9-1f61fb7d4c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 10, 'learning_rate': 0.01, 'max_depth': 1},\n",
       " 2.0103999999999993,\n",
       " 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified model to include tree depth simulation and grid search over hyperparameters\n",
    "\n",
    "class GradientBoostingRegressorScratchV2:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def _fit_base_model(self, residuals, depth):\n",
    "        # A simple decision stump (mean residual) that is adjusted to simulate tree depth\n",
    "        # For now, depth affects the fraction of the residual used in the model\n",
    "        model_value = np.mean(residuals) / (depth + 1)\n",
    "        return model_value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean prediction\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full_like(y, self.initial_prediction)\n",
    "        self.models = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            model = self._fit_base_model(residuals, self.max_depth)\n",
    "            self.models.append(model)\n",
    "            predictions += self.learning_rate * model\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model\n",
    "        return predictions\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_mse = float('inf')\n",
    "best_r2 = float('-inf')\n",
    "\n",
    "# Perform grid search over the parameters\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            model = GradientBoostingRegressorScratchV2(\n",
    "                n_estimators=n_estimators, \n",
    "                learning_rate=learning_rate, \n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "            \n",
    "            # Evaluate performance\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            r2 = r2_score(y, y_pred)\n",
    "            \n",
    "            # Track the best model based on MSE\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_r2 = r2\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "\n",
    "best_params, best_mse, best_r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af472e-39b1-4ef5-b03a-d53116e9e0ea",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d97a0f-7022-481e-9690-b2798389893b",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a machine learning model that performs slightly better than random guessing but is not highly accurate by itself. The idea is that these weak learners can be combined, or \"boosted,\" to create a strong learner that performs well overall.\n",
    "\n",
    "In the context of Gradient Boosting, the weak learners are typically shallow decision trees (also called decision stumps), which are decision trees with limited depth (often just one or a few levels). These trees capture simple patterns in the data, but on their own, they have limited predictive power.\n",
    "\n",
    "## Key Characteristics of Weak Learners:\n",
    "\n",
    "1. Low Complexity: Weak learners are intentionally kept simple to avoid overfitting. In Gradient Boosting, this is usually done by limiting the depth of the trees (e.g., max_depth = 1 or 2).\n",
    "\n",
    "2. Slightly Better than Random: A weak learner should do marginally better than random guessing (accuracy above 50% in classification, or reduced error in regression).\n",
    "\n",
    "3. Focus on Residuals: In each boosting iteration, the weak learner focuses on the residuals (errors) of the previous learners, correcting them step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97e7bf-3dff-41c4-8a1b-4ead02eb0cce",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6290b62a-998a-4fbc-a5fb-d58ce6864aaf",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is based on the idea of sequentially improving a model by focusing on the mistakes (residual errors) made by previous models. Here's how it works in simple terms:\n",
    "\n",
    "1. Start with a weak model:\n",
    "The process begins by fitting a very simple model, often referred to as a weak learner. In regression problems, this could be as simple as predicting the mean of the target values. This model usually has low accuracy and will leave a lot of residual errors (the difference between the actual target values and the predicted values).\n",
    "\n",
    "2. Focus on the errors:\n",
    "The key idea in Gradient Boosting is to focus on the data points where the initial model made the most significant mistakes. The algorithm fits a new weak learner (typically a decision tree) to predict the residual errors (the difference between the actual values and the model's current predictions).\n",
    "\n",
    "3. Correct the mistakes:\n",
    "The new weak learner tries to correct the errors made by the previous one. Instead of building a strong model all at once, Gradient Boosting gradually improves the overall performance by adding a sequence of weak learners, each focused on correcting the residuals from the previous ones.\n",
    "\n",
    "4. Iterate and combine:\n",
    "This process is repeated for a fixed number of iterations (or until the model stops improving). In each iteration, a new weak learner is trained on the residuals, and its contribution to the final model is controlled by a learning rate (a small fraction of the residual is added at each step).\n",
    "Each new model is added to the ensemble, slowly improving the overall prediction accuracy.\n",
    "\n",
    "5. Optimize via gradients:\n",
    "The term \"Gradient Boosting\" comes from the fact that this process can be viewed as minimizing a loss function (e.g., mean squared error for regression) by using gradient descent. In each step, the weak learner approximates the gradient of the loss function with respect to the current model's predictions, hence guiding the model in the right direction to reduce errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c8980-6476-41d7-a0b8-ad097342ca84",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
